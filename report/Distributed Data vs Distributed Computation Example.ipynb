{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51136792",
   "metadata": {},
   "source": [
    "# Distributed data vs distributed computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea407a",
   "metadata": {},
   "source": [
    "In this short, additional section we explore a distributed data paradigm vs a distributed computation paradigm through a working LLM summarisation framework, whilst thinking about applications in regards to the AI language detection problem as well. The data used is the same dataset throughout the project, found at https://www.kaggle.com/datasets/sunilthite/llm-detect-ai-generated-text-dataset/data. We also use data from a wikipedia article, detailed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41cd59a",
   "metadata": {},
   "source": [
    "### Exploring distributed data vs distributed computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b4d5b",
   "metadata": {},
   "source": [
    "Distributed data paradigms focus on breaking down large datasets into smaller chunks or partitions and distributing these segments across multiple computing nodes or storage systems. The primary goal is to distribute the data to make it accessible and processable across a distributed system. In comparison, distributed computation paradigms aim to perform computational tasks concurrently across multiple computing resources, leveraging parallel processing capabilities. The primary goal is to distribute computational tasks, allowing them to execute in parallel across a distributed infrastructure.\n",
    "\n",
    "Examples of distributed data paradigms are data partioning (splitting large texts into smaller segments for parallel processing), and distributed storage (storing data across multiple nodes using distributed file systems e.g. HDFS, AWS S3).\n",
    "\n",
    "Examples of distributed computation paradigms are MapReduce (dividing a computational task into smaller tasks *mapping* and aggregating the results *reducing* across distributed nodes).\n",
    "\n",
    "First, we want to download some libraries and load in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf074f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd9b0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"Training_Essay_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd1bc33",
   "metadata": {},
   "source": [
    "Data partitioning, a popular distriubted data method, can be easily applied to many different problem domains. Within the context of our problem we'd want to check if the size per document was large enough to warrant data partioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc1f8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                     text  generated\n",
       "0      Car-free cities have become a subject of incre...          1\n",
       "1      Car Free Cities  Car-free cities, a concept ga...          1\n",
       "2        A Sustainable Urban Future  Car-free cities ...          1\n",
       "3        Pioneering Sustainable Urban Living  In an e...          1\n",
       "4        The Path to Sustainable Urban Living  In an ...          1\n",
       "...                                                  ...        ...\n",
       "29140  There has been a fuss about the Elector Colleg...          0\n",
       "29141  Limiting car usage has many advantages. Such a...          0\n",
       "29142  There's a new trend that has been developing f...          0\n",
       "29143  As we all know cars are a big part of our soci...          0\n",
       "29144  Cars have been around since the 1800's and hav...          0\n",
       "\n",
       "[29145 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b66bbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4091"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = df.iloc[0,0]\n",
    "len(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffc779",
   "metadata": {},
   "source": [
    "Let's also have a look at the average length of the documents we have in the dataset we've used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946e34b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235.996740435752\n"
     ]
    }
   ],
   "source": [
    "average_length = df['text'].apply(lambda x: len(str(x))).mean()\n",
    "print(average_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaee1b0",
   "metadata": {},
   "source": [
    "We can see here, it's just over 2200 characters which is approximately 300-500 words. Whilst splitting up a document can significantly aid processing for AI detection, we'd more so see it being used on exceptionally large documents that may exceed the memory capacity of a single processing unit or, when considering the scalability of the task against the efficieny in a single computing unit.\n",
    "\n",
    "For example we might consider the case of an AI detection bot (among other functions) such as Turnitin, used widely across universities in the UK. It handles far larger documents (PhD's coming in at almost half a million characters) among a far larger volume of documents as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054dd25",
   "metadata": {},
   "source": [
    "### Intro to Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814d12e",
   "metadata": {},
   "source": [
    "Within the realm of Natural Language Processing (NLP), we highlight a solution to summarising extensive or multiple documents - an often formidable challenge when cosidering the sheer volumes of data recquired - which uses both distributed data and distributed computation techniques.\n",
    "\n",
    "Langchain is a solution to this problem that stores relevant information from previous documents within the current one, it establishes a comprehensive chain of interconnected documents. We'll see some of the processes it uses below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc83f1",
   "metadata": {},
   "source": [
    "First we want to set up the environment and install some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1dc921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tiktoken openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e2e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains import ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78de544",
   "metadata": {},
   "source": [
    "Initialise OpenAI Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cfd8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41000f6",
   "metadata": {},
   "source": [
    "Set up the summarisation chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f12c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo-16k\"\n",
    "temperature = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142b1a7",
   "metadata": {},
   "source": [
    "Upload the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ea06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "039dbcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://simple.wikipedia.org/wiki/Artificial_intelligence\")\n",
    "\n",
    "def load_folder(folder):\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_folder(loader)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a855a8c8",
   "metadata": {},
   "source": [
    "### Splitting Document Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc67fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87bfcc17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='At present we use the term AI for successfully understanding human speech,[6] competing at a high level in strategic game systems (such as chess and Go), self-driving cars, and interpreting complex data.[9] Some people also consider AI a danger to humanity if it continues to progress at its current pace.[10]\\nAn extreme goal of AI research is to create computer programs that can learn, solve problems, and think logically.[11][12] In practice, however, most applications have picked on problems which computers can do well. Searching databases and doing calculations are things computers do better than people. On the other hand, \"perceiving its environment\" in any real sense is way beyond present-day computing.', metadata={'source': 'https://simple.wikipedia.org/wiki/Artificial_intelligence', 'title': 'Artificial intelligence - Simple English Wikipedia, the free encyclopedia', 'language': 'en'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa7ef0",
   "metadata": {},
   "source": [
    "This is how Langchain applies its data partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eca11b",
   "metadata": {},
   "source": [
    "___\n",
    "In the domain of document processing, the practice of breaking extensive documents into manageable segments is a fundamnetal necessity. There are, equally important, functions of Langchain that effectively amalgamate these segments and summarises them as the principal function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c055f9",
   "metadata": {},
   "source": [
    "## Map Reduce Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435b3f5",
   "metadata": {},
   "source": [
    "So we've seen the use of distributed data in Langchain. Further along in the 'chains' it uses as part of the summarisation process, Langchain employs a distributed computation paradigm in its Map Reduce Chain.\n",
    "\n",
    "The Map Reduce Chain is a two-step solution that greatly simplifies the task of summarising a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34adc9",
   "metadata": {},
   "source": [
    "### Map Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c44ca3",
   "metadata": {},
   "source": [
    "In the first step, known as the \"map\", the document is divided into smaller, more manageable chunks. Each of these is then summarised individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1cb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_template = \"\"\"Write a concise summary of the following content:\n",
    "\n",
    "{text}\n",
    "\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67a01e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = PromptTemplate.from_template(map_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b459486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_chain = LLMChain(prompt=map_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a015b84",
   "metadata": {},
   "source": [
    "### Reduce Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab55c8",
   "metadata": {},
   "source": [
    "In the second step, referred to as the \"reduce\", we aim to combine these individual summaries into one cohesive final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec88ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "\n",
    "{doc_summaries}\n",
    "\n",
    "Summarize the above summaries with all the key details\n",
    "Summary:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4938c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93579dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86ad680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_chain = StuffDocumentsChain(llm_chain=reduce_chain, \n",
    "                                  document_variable_name=\"doc_summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ba5ef5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduce_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=stuff_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4b9f7",
   "metadata": {},
   "source": [
    "Attempting to merge all the chunk summaries can run into token limit contraints. A previous 'chain' combines chunks in such a way this is never reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25eac3",
   "metadata": {},
   "source": [
    "### Map Reduce Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b927aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    document_variable_name=\"text\",\n",
    "    reduce_documents_chain=reduce_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39dde08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = map_reduce_chain.run(docs)\n",
    "#wrapped_text = textwrap.fill(output, \n",
    "#                             width=100,\n",
    "#                             break_long_words=False,\n",
    "#                             replace_whitespace=False)\n",
    "#print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7679c1",
   "metadata": {},
   "source": [
    "A paid version of OpenAI's API is required to run with some Langchain functions however, the output it would produce is below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701968cf",
   "metadata": {},
   "source": [
    "___\n",
    "Artificial intelligence (AI) is a field of computer science that focuses on creating intelligent\n",
    "\n",
    "machines capable of tasks that typically require human intelligence. It is used in various\n",
    "\n",
    "industries and has the potential to greatly impact society. The term \"intelligence\" is debated in\n",
    "\n",
    "AI, with different perspectives on whether it should be defined in terms of action or thinking. AI\n",
    "\n",
    "involves computer programs that mimic human cognition and can learn, solve problems, and think\n",
    "\n",
    "logically. It is a multidisciplinary field that encompasses various disciplines. AI research began\n",
    "\n",
    "in 1956 but experienced a decline known as the \"AI winter\" before resurging in the 90s and early\n",
    "\n",
    "2000s. Notable achievements include machines defeating human champions in chess and Jeopardy! AI has\n",
    "\n",
    "become popular worldwide due to advancements in technology and access to more data. The content also\n",
    "\n",
    "includes references to various topics and sources discussing AI.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223574b",
   "metadata": {},
   "source": [
    "Map Reduce employs a smart iterative approach by progressively combining chunks, retaining the key information throughout the amalgamation process. The ability to perform computaional tasks concurrently across multiple computing resources allows Langchain to effectively summarise large documents far quicker than usual.\n",
    "\n",
    "When thinking back to our original problem, the key take-away here is scalability. Again, considering the Turnitin example where a programme would be processing an incredibly high volume of documents. Something else worth noting is reduced memory consumption that comes from distributed computation techniques by distributing memory requirements and thus, reducing hardware costs associated with language detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122261c1",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45354e60",
   "metadata": {},
   "source": [
    "In conclusion, we can respect the strength of both distributed data and distributed computation paradigms in their importance and respective uses through both the Langchain example and hypothesised use in AI language detection.\n",
    "\n",
    "It's worth noting their interdependence, in the way distributed computation often relies on the set up from distributed data paradigms to process data efficiently across distributed infrastructure.\n",
    "\n",
    "A worthy consideration is the requirement of very well-designed algorithms to be able to leverage distribution techniques effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
